{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlA6h6pyzQsX"
      },
      "outputs": [],
      "source": [
        "#@title Оптимизация модели - убираем REI\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder  # LabelEncoder для преобразования меток\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from imblearn.over_sampling import SMOTE  # Для балансировки данных\n",
        "from xgboost import XGBClassifier  # Используем XGBoost\n",
        "from collections import Counter\n",
        "\n",
        "# Путь к папке с JSON файлами\n",
        "output_dir = '/content/kata_elements_output'\n",
        "\n",
        "# Список индексов ключевых точек MediaPipe, которые нас интересуют\n",
        "required_indices = list(range(11, 17)) + list(range(23, 29))\n",
        "\n",
        "# Функция для вычисления угла между тремя точками\n",
        "def calculate_angle(a, b, c):\n",
        "    \"\"\"\n",
        "    Вычисляет угол между тремя точками a, b, c.\n",
        "    Точка b является центром угла.\n",
        "    \"\"\"\n",
        "    # Векторы от точки b к точкам a и c\n",
        "    ba = np.array(a) - np.array(b)\n",
        "    bc = np.array(c) - np.array(b)\n",
        "\n",
        "    # Косинус угла через скалярное произведение\n",
        "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
        "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))  # Ограничение значения косинуса\n",
        "    return np.degrees(angle)  # Преобразуем радианы в градусы\n",
        "\n",
        "# Функция для загрузки данных из JSON файлов с добавлением углов\n",
        "def load_data(output_dir):\n",
        "    X = []  # Признаки (координаты точек + углы)\n",
        "    y = []  # Метки классов (названия папок)\n",
        "\n",
        "    for class_name in os.listdir(output_dir):\n",
        "        if class_name.endswith('_poses_data.json'):\n",
        "            class_label = class_name.split('_')[0]  # Извлекаем название класса\n",
        "            json_path = os.path.join(output_dir, class_name)\n",
        "\n",
        "            with open(json_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            for pose_data in data:\n",
        "                landmarks = pose_data['landmarks']\n",
        "                # Создаем словарь для быстрого доступа к точкам\n",
        "                landmark_dict = {landmark['id']: (landmark['x'], landmark['y'], landmark['z']) for landmark in landmarks}\n",
        "\n",
        "                # Извлекаем координаты только для нужных точек\n",
        "                features = []\n",
        "                for i in required_indices:\n",
        "                    if i in landmark_dict:\n",
        "                        features.extend(landmark_dict[i])\n",
        "                    else:\n",
        "                        features.extend([0, 0, 0])  # Если точка не найдена, добавляем нули\n",
        "\n",
        "                # Добавляем углы между точками\n",
        "                angles = [\n",
        "                    calculate_angle(landmark_dict.get(16, (0, 0, 0)), landmark_dict.get(14, (0, 0, 0)), landmark_dict.get(12, (0, 0, 0))),\n",
        "                    calculate_angle(landmark_dict.get(14, (0, 0, 0)), landmark_dict.get(12, (0, 0, 0)), landmark_dict.get(24, (0, 0, 0))),\n",
        "                    calculate_angle(landmark_dict.get(14, (0, 0, 0)), landmark_dict.get(12, (0, 0, 0)), landmark_dict.get(11, (0, 0, 0))),\n",
        "\n",
        "                    calculate_angle(landmark_dict.get(15, (0, 0, 0)), landmark_dict.get(13, (0, 0, 0)), landmark_dict.get(11, (0, 0, 0))),\n",
        "                    calculate_angle(landmark_dict.get(13, (0, 0, 0)), landmark_dict.get(11, (0, 0, 0)), landmark_dict.get(12, (0, 0, 0))),\n",
        "                    calculate_angle(landmark_dict.get(13, (0, 0, 0)), landmark_dict.get(11, (0, 0, 0)), landmark_dict.get(23, (0, 0, 0))),\n",
        "\n",
        "                    calculate_angle(landmark_dict.get(28, (0, 0, 0)), landmark_dict.get(26, (0, 0, 0)), landmark_dict.get(24, (0, 0, 0))),\n",
        "                    calculate_angle(landmark_dict.get(26, (0, 0, 0)), landmark_dict.get(24, (0, 0, 0)), landmark_dict.get(12, (0, 0, 0))),\n",
        "                    calculate_angle(landmark_dict.get(26, (0, 0, 0)), landmark_dict.get(24, (0, 0, 0)), landmark_dict.get(23, (0, 0, 0))),\n",
        "\n",
        "                    calculate_angle(landmark_dict.get(27, (0, 0, 0)), landmark_dict.get(25, (0, 0, 0)), landmark_dict.get(23, (0, 0, 0))),\n",
        "                    calculate_angle(landmark_dict.get(25, (0, 0, 0)), landmark_dict.get(23, (0, 0, 0)), landmark_dict.get(24, (0, 0, 0))),\n",
        "                    calculate_angle(landmark_dict.get(25, (0, 0, 0)), landmark_dict.get(23, (0, 0, 0)), landmark_dict.get(11, (0, 0, 0)))\n",
        "                ]\n",
        "                features.extend(angles)\n",
        "\n",
        "                X.append(features)\n",
        "                y.append(class_label)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Загружаем данные\n",
        "X, y = load_data(output_dir)\n",
        "\n",
        "# Исключаем указанные классы\n",
        "classes_to_exclude = [\n",
        "    \"l-chudan-gyaku-tsuki\",\n",
        "    \"l-chudan-mae-geri\",\n",
        "    \"l-gedan-barai\",\n",
        "    \"l-jodan-age-uke\",\n",
        "    \"r-chudan-mae-geri\",\n",
        "    \"r-gedan-barai\",\n",
        "    \"r-jodan-age-uke\",\n",
        "    \"r-tsuru-ashi-dachi\"\n",
        "    \"l-chudan-gyaku-uchi-uke\",\n",
        "    \"l-chudan-shuto-uke\",\n",
        "    \"r-chudan-shuto-uke\",\n",
        "    \"l-turn-180\",\n",
        "    \"l-turn-90\",\n",
        "    \"l-zenkutsu-dachi-hanmi\",\n",
        "    \"r-chudan-gyaku-uchi-uke\",\n",
        "    \"r-chudan-nukite\",\n",
        "    \"r-chudan-yoko-tettsui\",\n",
        "    \"r-jodan-uraken\",\n",
        "    \"r-jodan-yoko-keage\",\n",
        "    \"r-morote-uke\",\n",
        "    \"r-turn-180\",\n",
        "    \"r-turn-90\",\n",
        "    \"r-zenkutsu-dachi-hanmi\",\n",
        "    \"l-chudan-gyaku-uchi-uke\",\n",
        "    \"r-tsuru-ashi-dachi\",\n",
        "    \"l-chudan-tettsui\",\n",
        "    \"l-jodan-haiwan-uke\",\n",
        "    #\"l-jodan-shuto-age-uke\",\n",
        "    #\"l-step-45\",\n",
        "    \"l-turn-270\",\n",
        "    \"l-zenkutsu-dachi\",\n",
        "    \"r-chudan-tettsui\",\n",
        "    \"l-chudan-osae-uke\",\n",
        "    \"r-chudan-gyaku-tsuki\",\n",
        "    \"r-turn-135\",\n",
        "    \"l-turn-45\",\n",
        "    \"r-zenkutsu-dach\",\n",
        "    \"l-chudan-yoko-tettsui\",\n",
        "    \"r-zenkutsu-dachi\",\n",
        "    \"r-jodan-haiwan-uke\",\n",
        "    #\"r-step-fwd\"\n",
        "    \"rei\"\n",
        "]\n",
        "mask = ~np.isin(y, classes_to_exclude)\n",
        "X = X[mask]\n",
        "y = y[mask]\n",
        "\n",
        "# Преобразуем метки классов в числовые значения с помощью LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Разделяем данные на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "# Нормализация данных\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Балансировка данных с помощью SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Создаём и обучаем модель (Random Forest) с учётом весов классов\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    class_weight='balanced'  # Учёт весов классов\n",
        ")\n",
        "model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Оцениваем модель на тестовой выборке\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Выводим отчёт о классификации с параметром zero_division\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_pred,\n",
        "    target_names=label_encoder.classes_,  # Возвращаем оригинальные метки классов\n",
        "    zero_division=0  # Устанавливаем значение метрик равным 0 при делении на ноль\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Сохраняем модель в формате .pkl\n",
        "model_path = '/content/drive/MyDrive/Karate_2/dataset/random_forest_model_9_no_rei.pkl'\n",
        "with open(model_path, 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "print(f\"Модель успешно сохранена в файл: {model_path}\")\n",
        "\n",
        "# Сохраняем LabelEncoder\n",
        "label_encoder_path = '/content/drive/MyDrive/Karate_2/dataset/label_encoder_9_no_rei.pkl'\n",
        "with open(label_encoder_path, 'wb') as file:\n",
        "    pickle.dump(label_encoder, file)\n",
        "\n",
        "# Сохраняем StandardScaler\n",
        "scaler_path = '/content/drive/MyDrive/Karate_2/dataset/scaler_9_no_rei.pkl'\n",
        "with open(scaler_path, 'wb') as file:\n",
        "    pickle.dump(scaler, file)\n",
        "\n",
        "print(\"LabelEncoder и StandardScaler успешно сохранены.\")"
      ],
      "metadata": {
        "id": "nKvBSFYdzUSV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}